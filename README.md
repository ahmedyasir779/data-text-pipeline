# ğŸ”— Unified Data & Text Processing Pipeline

> Production-grade data analytics platform combining structured data processing with advanced NLP.
## ğŸŒ **LIVE DEMO**

**ğŸ‘‰ Try it now: [https://data-text-pipeline.onrender.com](https://data-text-pipeline.onrender.com)**

No installation required - just upload your data and analyze!


---

![Python Version](https://img.shields.io/badge/python-3.8%2B-blue)
![License](https://img.shields.io/badge/license-MIT-green)
![Tests](https://img.shields.io/badge/tests-passing-brightgreen)
![Coverage](https://img.shields.io/badge/coverage-75%25-yellow)
![Deployment](https://img.shields.io/badge/deployment-live-success)

[Live Demo](https://data-text-pipeline.onrender.com) | [Documentation](docs/) | [API Reference](docs/API.md)

---



## ğŸ“– Table of Contents

- [Overview](#overview)
- [Features](#features)
- [Quick Start](#quick-start)
- [Usage Examples](#usage-examples)
- [Architecture](#architecture)
- [Performance](#performance)
- [Documentation](#documentation)
- [Testing](#testing)
- [Contributing](#contributing)
- [License](#license)

---

## ğŸ¯ Overview

The Unified Data & Text Processing Pipeline is an all-in-one analytics platform that seamlessly combines:

- **Structured Data Processing** - Handle CSV, Excel, JSON files
- **Natural Language Processing** - Advanced text analysis with sentiment, entities, topics
- **Intelligent Caching** - 3-10x performance boost on repeated operations
- **Production-Ready** - Comprehensive testing, logging, and error handling

**Perfect for:** Data scientists, analysts, researchers, and developers working with mixed data types.

---

## âœ¨ Features

### ğŸ“Š Data Processing
- âœ… Multi-format support (CSV, Excel, JSON)
- âœ… Intelligent cleaning (missing values, duplicates, outliers)
- âœ… Statistical analysis (mean, median, correlations)
- âœ… Flexible cleaning strategies

### ğŸ“ Text Processing
- âœ… Sentiment analysis (polarity & subjectivity)
- âœ… Named Entity Recognition (people, organizations, locations)
- âœ… Keyword extraction (RAKE & TF-IDF algorithms)
- âœ… Topic detection (automatic categorization)
- âœ… Readability analysis (Flesch scores)

### âš¡ Performance
- âœ… Smart caching system (3-10x speedup)
- âœ… Progress tracking for long operations
- âœ… Batch processing support
- âœ… Configuration file support (JSON/YAML)

### ğŸ“ˆ Visualization
- âœ… 6-panel analytics dashboard
- âœ… Sentiment distribution charts
- âœ… Correlation scatter plots
- âœ… Word frequency visualizations

### ğŸ”§ Production Features
- âœ… REST API ready
- âœ… Docker support
- âœ… Comprehensive logging
- âœ… 30+ unit tests (75% coverage)
- âœ… Type hints throughout
- âœ… Error recovery

---

## ğŸš€ Quick Start

### Installation
```bash
# Clone repository
git clone https://github.com/ahmedyasir779/data-text-pipeline.git
cd data-text-pipeline

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Download NLP models
python -m spacy download en_core_web_sm
python -m textblob.download_corpora
```
## ğŸ³ Docker Compose (Easiest Way)

### Quick Start
```bash
# Clone repository
git clone https://github.com/YOUR-USERNAME/data-text-pipeline.git
cd data-text-pipeline

# Start the app
docker-compose up

# Open browser: http://localhost:8501
```

### Commands
```bash
# Start (foreground)
docker-compose up

# Start (background)
docker-compose up -d

# Stop
docker-compose down

# View logs
docker-compose logs -f

# Rebuild after code changes
docker-compose up --build
```

### What it does

- Pulls Docker image from Docker Hub
- Mounts your local data/ and output/ directories
- Exposes web interface on port 8501
- Automatically restarts if it crashes

That's it! No complex setup needed.
--------------------------------------------------------------


### Basic Usage
```bash
# Quick analysis of CSV with text column
python cli.py --data-file data.csv --text-column reviews --all

# Output:
# âœ“ Loaded 1000 rows
# âœ“ Sentiment: 65% positive, 20% neutral, 15% negative
# âœ“ Found 45 organizations, 23 locations
# âœ“ Dashboard saved to output/analysis_dashboard.png
```

---

## ğŸ“– Usage Examples

### Example 1: Customer Review Analysis
```bash
python cli.py \
  --data-file customer_reviews.csv \
  --text-column review \
  --correlate rating \
  --all

# Analyzes:
# - Review sentiment vs ratings
# - Common keywords and topics
# - Named entities (products, companies)
# - Text complexity
```

**Output:**
- Statistical analysis of ratings
- Sentiment breakdown
- Top keywords
- Correlation: rating vs review length & sentiment
- Visual dashboard

### Example 2: As Python Library
```python
from unified_pipeline import UnifiedPipeline

# Initialize
pipeline = UnifiedPipeline(use_cache=True)

# Process data
pipeline.load_structured_data('sales.csv')
pipeline.load_text_column('customer_feedback')

# Analyze with method chaining
(pipeline
 .clean_data()
 .clean_text()
 .analyze_data()
 .analyze_sentiment()
 .extract_entities()
 .extract_keywords(method='tfidf')
 .detect_topics()
 .create_visualizations()
 .export_results(format='csv'))

# Get results
report = pipeline.generate_report()
print(report)
```

### Example 3: Batch Processing
```bash
# Process all CSV files in directory
python cli.py \
  --batch data/monthly_reports/ \
  --text-column comments \
  --all

# Progress: Processing files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [01:24<00:00]
# Result: Processed 12/12 files successfully
```

### Example 4: Configuration File

**config.json:**
```json
{
  "pipeline": {
    "use_cache": true,
    "clean_strategy": "drop",
    "analyze_sentiment": true,
    "extract_entities": true,
    "extract_keywords": true
  },
  "output": {
    "directory": "results",
    "format": "csv"
  }
}
```

**Run:**
```bash
python cli.py --data-file data.csv --text-column text --config config.json
```

---

## ğŸ—ï¸ Architecture
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              CLI Interface                      â”‚
â”‚  (Command-line arguments, config files)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Unified Pipeline Core                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ Data Loading      â€¢ Text Loading             â”‚
â”‚  â€¢ Cleaning          â€¢ Analysis                 â”‚
â”‚  â€¢ Method Chaining   â€¢ Result Aggregation       â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚         â”‚          â”‚          â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Data  â”‚ â”‚ Text  â”‚ â”‚Advancedâ”‚ â”‚Visualizationâ”‚
â”‚Processorâ”‚ Processor  NLP   â”‚ â”‚ Generator  â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚        â”‚          â”‚          â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Support Modules                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Cache Manager    â€¢ Progress Tracker       â”‚
â”‚ â€¢ Config Manager   â€¢ Logger                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Module Overview

| Module | Purpose | Key Features |
|--------|---------|--------------|
| `unified_pipeline.py` | Core orchestration | Method chaining, result aggregation |
| `advanced_nlp.py` | NLP processing | Entities, keywords, topics, readability |
| `cache_manager.py` | Performance optimization | Hash-based caching, category storage |
| `progress_tracker.py` | User feedback | Progress bars, time estimates |
| `config_manager.py` | Configuration | JSON/YAML support |

---

## âš¡ Performance

### Caching Impact

Tested on dataset with 50 rows:

| Operation | No Cache | With Cache | Speedup |
|-----------|----------|------------|---------|
| Load Data | 0.15s | 0.02s | **7.5x** |
| Sentiment Analysis | 2.30s | 0.20s | **11.5x** |
| Entity Extraction | 3.10s | 0.30s | **10.3x** |
| Keyword Extraction | 1.80s | 0.15s | **12.0x** |
| **Total Pipeline** | **8.50s** | **0.85s** | **10.0x** |

### Cache Management
```bash
# Clear all caches
python cli.py --clear-cache

# Disable cache for a run
python cli.py --no-cache --data-file data.csv --text-column text --all

# Check cache size
python -c "from cache_manager import CacheManager; print(CacheManager().get_cache_size())"
```

### Batch Processing Performance

Processing 10 files (500 rows each):
- **Sequential:** ~85 seconds
- **With caching (second run):** ~8 seconds
- **Speedup:** 10.6x faster

---

## ğŸ“š Documentation

### Full Documentation

- [Installation Guide](docs/INSTALLATION.md)
- [API Reference](docs/API.md)
- [Usage Examples](docs/EXAMPLES.md)
- [Configuration Guide](docs/CONFIG.md)
- [Testing Guide](TESTING.md)
- [Logging Guide](LOGGING.md)
- [Contributing Guide](CONTRIBUTING.md)

### Command-Line Reference
```bash
# Input
--data-file, -d          Input data file (CSV/Excel/JSON)
--text-column, -t        Column containing text
--text-file              Separate text file

# Processing
--clean, -c              Clean data and text
--clean-strategy         Strategy: drop, fill, forward_fill

# Analysis
--analyze-data           Analyze structured data
--analyze-text           Analyze text data
--sentiment              Sentiment analysis
--entities               Named entity recognition
--keywords               Keyword extraction (rake/tfidf)
--topics                 Topic detection
--complexity             Readability analysis
--correlate              Correlate column with text

# Visualization
--visualize              Create dashboard

# Export
--export                 Export format (csv/json/excel)
--output, -o             Output directory

# Performance
--no-cache               Disable caching
--clear-cache            Clear cache before run
--batch                  Batch process directory

# Configuration
--config                 Load from config file
--all, -a                Run complete pipeline
```

---

## ğŸ§ª Testing

### Run Tests
```bash
# All tests
pytest tests/ -v

# With coverage
pytest tests/ --cov=. --cov-report=html

# Specific module
pytest tests/test_unified_pipeline.py -v
```

### Test Coverage
```
tests/
â”œâ”€â”€ conftest.py              # Fixtures
â”œâ”€â”€ test_unified_pipeline.py # 15 tests
â”œâ”€â”€ test_cache_manager.py    # 8 tests
â””â”€â”€ test_advanced_nlp.py     # 9 tests

Total: 32 tests
Coverage: 75%
Status: All passing âœ…
```

### Continuous Integration

Tests run automatically on:
- Every push to dev/main branches
- Pull requests
- Before deployment

---

## ğŸ¤ Contributing

We welcome contributions! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

### Quick Contribution Guide

1. Fork the repository
2. Create feature branch (`git checkout -b feature/amazing-feature`)
3. Make changes and add tests
4. Ensure tests pass (`pytest tests/`)
5. Commit (`git commit -m 'Add amazing feature'`)
6. Push (`git push origin feature/amazing-feature`)
7. Open Pull Request

### Development Setup
```bash
# Clone your fork
git clone https://github.com/ahmedyasir779/data-text-pipeline.git
cd data-text-pipeline

# Create dev environment
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
pip install -r requirements-dev.txt  # Testing tools

# Run tests
pytest tests/ -v
```

---

## ğŸ“Š Project Stats

- **Lines of Code:** ~2,500
- **Modules:** 8 core modules
- **Tests:** 32 unit tests
- **Documentation:** 6 guides
- **Dependencies:** 15 packages
- **Python Version:** 3.8+

---

## ğŸ—ºï¸ Roadmap

### Completed âœ…
- [x] Unified data & text pipeline
- [x] Sentiment analysis
- [x] Named entity recognition
- [x] Keyword extraction & topic detection
- [x] Caching system
- [x] Batch processing
- [x] Configuration files
- [x] Comprehensive testing

### Planned ğŸš§
- [ ] REST API endpoint
- [ ] Docker container
- [ ] Web dashboard (Streamlit)
- [ ] Real-time processing
- [ ] Multi-language support
- [ ] Custom ML model integration

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ‘¤ Author

**Ahmed Yasir**
- GitHub: [@ahmedyasir779](https://github.com/ahmedyasir779)
- LinkedIn: [ahmed-yasir-907561206](www.linkedin.com/in/ahmed-yasir-907561206)

---

## ğŸ™ Acknowledgments

- Built as part of a 4-month AI/ML engineering learning journey
- Week 3 of 16-week roadmap
- Integrated: Data Processor (Week 1) + Text Processor (Week 2)

---

## ğŸ“ˆ Used By

*Add companies/projects using this tool here*

---

## â­ Star History

[![Star History Chart](https://api.star-history.com/svg?repos=ahmedyasir779/data-text-pipeline&type=Date)](https://star-history.com/#ahmedyasir779/data-text-pipeline&Date)

---

## ğŸ’¬ Support

- ğŸ“§ Email: your.email@example.com
- ğŸ’¬ Discussions: [GitHub Discussions](https://github.com/ahmedyasir779/data-text-pipeline/discussions)
- ğŸ› Issues: [GitHub Issues](https://github.com/ahmedyasir779/data-text-pipeline/issues)

---

<p align="center">
  <strong>If this project helped you, please â­ star it on GitHub!</strong>
</p>

<p align="center">
  Made with â¤ï¸ and Python
</p>